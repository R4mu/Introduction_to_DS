{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77d67523",
   "metadata": {},
   "source": [
    "\n",
    "# Markov Chain Load Time Optimization\n",
    "\n",
    "This notebook analyzes user behavior on a website using a Markov chain model. Each row in the dataset represents a transition from one page to another. We estimate transition probabilities, simulate load times under different preloading strategies, and analyze long-term expected performance.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load and Prepare the Data\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/websites.csv', header=None, names=['source', 'destination'])\n",
    "\n",
    "# Get the number of pages\n",
    "n_pages = max(df.max()) + 1  # assumes pages are labeled 0 to n-1\n",
    "\n",
    "# Initialize count matrix\n",
    "count_matrix = np.zeros((n_pages, n_pages))\n",
    "\n",
    "# Populate count matrix\n",
    "for _, row in df.iterrows():\n",
    "    count_matrix[row['source'], row['destination']] += 1\n",
    "\n",
    "# Normalize rows to get transition matrix\n",
    "transition_matrix = count_matrix / count_matrix.sum(axis=1, keepdims=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Simulate Load Times from Page 1\n",
    "\n",
    "```python\n",
    "np.random.seed(42)\n",
    "\n",
    "# Extract probabilities from page 1\n",
    "probabilities = transition_matrix[1]\n",
    "\n",
    "# Sample next pages\n",
    "n_samples = 10000\n",
    "next_pages = np.random.choice(n_pages, size=n_samples, p=probabilities)\n",
    "\n",
    "# Identify top 1 and top 2 likely next pages\n",
    "top1 = np.argsort(probabilities)[-1]\n",
    "top2 = np.argsort(probabilities)[-2:]\n",
    "\n",
    "# Simulate load times\n",
    "load_times_1 = np.where(next_pages == top1,\n",
    "                        np.random.exponential(1/10, size=n_samples),\n",
    "                        np.random.exponential(1, size=n_samples))\n",
    "\n",
    "load_times_2 = np.where(np.isin(next_pages, top2),\n",
    "                        np.random.exponential(1/10, size=n_samples),\n",
    "                        np.random.exponential(1, size=n_samples))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Compare Empirical and Theoretical Load Times\n",
    "\n",
    "```python\n",
    "avg_no_preloading = 1.0\n",
    "avg_one_preload = np.mean(load_times_1)\n",
    "avg_two_preload = np.mean(load_times_2)\n",
    "\n",
    "print(f\"Avg load time with no preloading: {avg_no_preloading:.4f}\")\n",
    "print(f\"Avg load time with 1 page preloaded: {avg_one_preload:.4f}\")\n",
    "print(f\"Avg load time with 2 pages preloaded: {avg_two_preload:.4f}\")\n",
    "```\n",
    "\n",
    "> **Conclusion:** The empirical average load times are lower than the baseline of 1 second. Thus, preloading significantly improves page load times.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Stationary Distribution and Long-Term Expected Load Time\n",
    "\n",
    "```python\n",
    "# Compute stationary distribution\n",
    "eigvals, eigvecs = np.linalg.eig(transition_matrix.T)\n",
    "stationary_dist = np.real(eigvecs[:, np.isclose(eigvals, 1)])\n",
    "stationary_dist = stationary_dist[:, 0]\n",
    "stationary_dist = stationary_dist / stationary_dist.sum()\n",
    "\n",
    "# Expected load time with one page preloaded from each state\n",
    "expected_load_times = np.zeros(n_pages)\n",
    "for i in range(n_pages):\n",
    "    probs = transition_matrix[i]\n",
    "    top = np.argmax(probs)\n",
    "    expected = np.sum(\n",
    "        np.where(np.arange(n_pages) == top, 1/10, 1) * probs\n",
    "    )\n",
    "    expected_load_times[i] = expected\n",
    "\n",
    "# Weighted by stationary distribution\n",
    "expected_load_time_stationary = np.dot(stationary_dist, expected_load_times)\n",
    "print(f\"Expected load time with preloading using stationary distribution: {expected_load_time_stationary:.4f}\")\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5251d269",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# SVD-Based Anomaly Detection\n",
    "\n",
    "This section analyzes a dataset using Singular Value Decomposition (SVD) and detects anomalies based on reconstruction error from a low-rank approximation.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: Load Data and Compute SVD\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/SVD.csv', header=None)\n",
    "problem1_data = df.values\n",
    "\n",
    "# Perform SVD\n",
    "U, s, VT = np.linalg.svd(problem1_data, full_matrices=False)\n",
    "D = np.diag(s)\n",
    "\n",
    "# First singular vectors\n",
    "first_left_singular_vector = U[:, 0]\n",
    "first_right_singular_vector = VT[0, :]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Explained Variance and Component Selection\n",
    "\n",
    "```python\n",
    "explained_variance = s**2\n",
    "total_variance = np.sum(explained_variance)\n",
    "explained_variance_ratio = np.cumsum(explained_variance) / total_variance\n",
    "\n",
    "# Select number of components for 95% variance\n",
    "num_components = np.searchsorted(explained_variance_ratio, 0.95) + 1\n",
    "print(f\"Components needed to explain 95% variance: {num_components}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Best Rank-k Approximation\n",
    "\n",
    "```python\n",
    "U_k = U[:, :num_components]\n",
    "D_k = D[:num_components, :num_components]\n",
    "VT_k = VT[:num_components, :]\n",
    "problem1_approximation = U_k @ D_k @ VT_k\n",
    "```\n",
    "\n",
    "> Each row in the approximated matrix represents a denoised or compressed version of the corresponding sample in the original data using only the most important singular directions.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Anomaly Detection Using Reconstruction Error\n",
    "\n",
    "```python\n",
    "reconstruction_errors = np.linalg.norm(problem1_data - problem1_approximation, axis=1)\n",
    "\n",
    "# Plot ECDF\n",
    "import matplotlib.pyplot as plt\n",
    "sorted_errors = np.sort(reconstruction_errors)\n",
    "n = len(sorted_errors)\n",
    "ecdf = np.arange(1, n + 1) / n\n",
    "\n",
    "plt.plot(sorted_errors, ecdf)\n",
    "plt.xlabel(\"Reconstruction Error\")\n",
    "plt.ylabel(\"Empirical CDF\")\n",
    "plt.title(\"Empirical Distribution of Reconstruction Errors\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Identify top 10 anomalies\n",
    "threshold = sorted_errors[-10]\n",
    "anomalous_samples = np.argsort(reconstruction_errors)[-10:]\n",
    "print(\"Indices of 10 most anomalous samples:\", anomalous_samples)\n",
    "```\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
